{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qIwoQi_ZJOPD"
   },
   "source": [
    "# Stanford CME 241 (Winter 2026) - Assignment 1\n",
    "\n",
    "**Due: Friday, January 23 @ 11:59 PM PST on Gradescope.**\n",
    "\n",
    "Assignment instructions:\n",
    "- Make sure each of the subquestions have answers\n",
    "- Ensure that group members indicate which problems they're in charge of\n",
    "- Show work and walk through your thought process where applicable\n",
    "- Empty code blocks are for your use, so feel free to create more under each section as needed\n",
    "- Document code with light comments (i.e. 'this function handles visualization')\n",
    "\n",
    "Submission instructions:\n",
    "- When complete, fill out your publicly available GitHub repo file URL and group members below, then export or print this .ipynb file to PDF and upload the PDF to Gradescope.\n",
    "\n",
    "*Link to this ipynb file in your public GitHub repo (replace below URL with yours):*\n",
    "\n",
    "https://github.com/onat-dalmaz/RL-book/blob/main/assignment1.ipynb\n",
    "\n",
    "*Group members (replace below names with people in your group):*\n",
    "- Onat Dalmaz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZupXsgsIJOPE"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T18:08:13.649744Z",
     "iopub.status.busy": "2026-01-22T18:08:13.649572Z",
     "iopub.status.idle": "2026-01-22T18:08:14.069202Z",
     "shell.execute_reply": "2026-01-22T18:08:14.068732Z"
    },
    "id": "F-w4vzDkJOPF"
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from typing import Dict, Mapping, List, Tuple\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# RL-book imports (installed via Assignment 0). If these fail, ensure you activated the same venv.\n",
    "from rl.distribution import Categorical, Constant\n",
    "from rl.markov_process import FiniteMarkovProcess, NonTerminal, Terminal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcvIKfARJOPF"
   },
   "source": [
    "## Question 1: Snakes and Ladders\n",
    "\n",
    "In the classic childhood game of Snakes and Ladders, all players start to the left of square 1 (call this position 0) and roll a 6-sided die to represent the number of squares they can move forward. The goal is to reach square 100 as quickly as possible. Landing on the bottom rung of a ladder allows for an automatic free-pass to climb, e.g. square 4 sends you directly to 14; whereas landing on a snake's head forces one to slide all the way to the tail, e.g. square 34 sends you to 6. Note, this game can be viewed as a Markov Process, where the outcome is only depedent on the current state and not the prior trajectory. In this question, we will ask you to both formally describe the Markov Process that describes this game, followed by coding up a version of the game to get familiar with the RL-book libraries.\n",
    "\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "How can we model this problem with a Markov Process?\n",
    "\n",
    "---\n",
    "\n",
    "### Subquestions\n",
    "\n",
    "#### Part (A): MDP Modeling\n",
    "\n",
    "Formalize the state space of the Snakes and Ladders game. Don't forget to specify the terminal state!\n",
    "\n",
    "---\n",
    "\n",
    "#### Part (B): Transition Probabilities\n",
    "\n",
    "Write out the structure of the transition probabilities. Feel free to abbreviate all squares that do not have a snake or ladder.\n",
    "\n",
    "---\n",
    "\n",
    "#### Part (C): Modeling the Game\n",
    "\n",
    "Code up a `transition_map: Transition[S]` data structure to represent the transition probabilities of the Snakes and Ladders Markov Process so you can model the game as an instance of `FiniteMarkovProcess`. Use the `traces` method to create sampling traces, and plot the graph of the distribution of time steps to finish the game. Use the image provided for the locations of the snakes and ladders.\n",
    "\n",
    "https://drive.google.com/file/d/1yhP242sG092Ico_WOPKrUp8jVJHbuGHH/view?usp=sharing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2VHXTG2JOPG"
   },
   "source": [
    "### Part (A) Answer\n",
    "\n",
    "A convenient Markov Process model uses one state per board position.\n",
    "\n",
    "- **State space**: \\(S = \\{0,1,2,\\ldots,100\\}\\), where state \\(s\\) means “the token is on square \\(s\\)”.\n",
    "- **Terminal state**: \\(100\\) is terminal (absorbing): once you reach square 100, the game ends.\n",
    "\n",
    "To incorporate snakes/ladders, define a deterministic “jump” function \\(J:\\{0,\\ldots,100\\}\\to\\{0,\\ldots,100\\}\\) that maps\n",
    "a square to its post-jump destination (identity on ordinary squares, head→tail for snakes, bottom→top for ladders).\n",
    "Then the one-step evolution is: roll a die \\(D\\in\\{1,\\ldots,6\\}\\), move to \\(\\min(100, s + D)\\), then apply \\(J\\).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-iQn1b0OJOPG"
   },
   "source": [
    "### Part (B) Answer\n",
    "\n",
    "Let \\(D\\sim\\text{Unif}\\{1,2,3,4,5,6\\}\\). For any non-terminal square \\(s\\in\\{0,\\ldots,99\\}\\), define\n",
    "\n",
    "\\[\n",
    "s' = J\\big(\\min(100, s + D)\\big).\n",
    "\\]\n",
    "\n",
    "Then the transition probabilities are\n",
    "\n",
    "\\[\n",
    "\\mathbb P[X_{t+1}=x\\mid X_t=s]\n",
    "= \\frac{1}{6}\\,\\big|\\{d\\in\\{1,\\ldots,6\\}: J(\\min(100,s+d)) = x\\}\\big|.\n",
    "\\]\n",
    "\n",
    "For the terminal state:\n",
    "\n",
    "\\[\n",
    "\\mathbb P[X_{t+1}=100\\mid X_t=100] = 1.\n",
    "\\]\n",
    "\n",
    "Equivalently: **each die face contributes probability \\(1/6\\)** to the single next square produced by “move then jump”.\n",
    "Squares without snakes/ladders simply have \\(J(u)=u\\).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mAC0uAmpJOPH"
   },
   "source": [
    "### Part (C) Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T18:08:14.070687Z",
     "iopub.status.busy": "2026-01-22T18:08:14.070580Z",
     "iopub.status.idle": "2026-01-22T18:08:14.769870Z",
     "shell.execute_reply": "2026-01-22T18:08:14.769516Z"
    },
    "id": "98uhXDn6JOPH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=30000  mean=23.90  median=22  p90=36\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Snakes & Ladders transition model (FiniteMarkovProcess) + finish-time histogram\n",
    "\n",
    "# Standard 100-square Snakes & Ladders board (Milton Bradley / common Wikipedia version).\n",
    "# If your provided image differs, edit this dict to match the snake/ladder endpoints.\n",
    "JUMPS: Dict[int, int] = {\n",
    "    # ladders (bottom -> top)\n",
    "    4: 24,\n",
    "    8: 29,\n",
    "    15: 35,\n",
    "    22: 40,\n",
    "    34: 54,\n",
    "    42: 61,\n",
    "    47: 67,\n",
    "    53: 73,\n",
    "    66: 85,\n",
    "    68: 87,\n",
    "    82: 99,\n",
    "    89: 92,\n",
    "    # snakes (head -> tail)\n",
    "    26: 6,\n",
    "    32: 12,\n",
    "    48: 28,\n",
    "    56: 36,\n",
    "    63: 58,\n",
    "    88: 72,\n",
    "    95: 75,\n",
    "    97: 77,\n",
    "}\n",
    "\n",
    "def apply_jump(pos: int) -> int:\n",
    "    \"\"\"Apply snake/ladder jump if present; otherwise identity.\"\"\"\n",
    "    return JUMPS.get(pos, pos)\n",
    "\n",
    "def next_pos(s: int, die: int) -> int:\n",
    "    \"\"\"One move: advance by die (cap at 100), then apply jump.\"\"\"\n",
    "    return apply_jump(min(100, s + die))\n",
    "\n",
    "# Build the transition map for non-terminal states (0..99). Terminal state 100 is omitted.\n",
    "transition_map: Dict[int, Categorical[int]] = {}\n",
    "for s in range(0, 100):\n",
    "    outcomes = [next_pos(s, d) for d in range(1, 7)]\n",
    "    probs: Dict[int, float] = {}\n",
    "    for x in outcomes:\n",
    "        probs[x] = probs.get(x, 0.0) + 1.0 / 6.0\n",
    "    transition_map[s] = Categorical(probs)\n",
    "\n",
    "mp = FiniteMarkovProcess(transition_map)\n",
    "\n",
    "def sample_game_length(num_games: int = 50_000) -> np.ndarray:\n",
    "    \"\"\"Sample episode lengths (number of die rolls) to reach Terminal(100).\"\"\"\n",
    "    lengths: List[int] = []\n",
    "    start_dist = Constant(NonTerminal(0))\n",
    "    for _ in range(num_games):\n",
    "        steps = 0\n",
    "        for st in mp.simulate(start_dist):\n",
    "            if isinstance(st, Terminal):\n",
    "                break\n",
    "            steps += 1\n",
    "        lengths.append(steps)\n",
    "    return np.array(lengths, dtype=int)\n",
    "\n",
    "lengths = sample_game_length(num_games=30_000)\n",
    "\n",
    "print(f\"samples={len(lengths)}  mean={lengths.mean():.2f}  median={np.median(lengths):.0f}  p90={np.quantile(lengths, 0.9):.0f}\")\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(lengths, bins=60)\n",
    "plt.title(\"Snakes & Ladders: distribution of rolls to finish\")\n",
    "plt.xlabel(\"# rolls to reach square 100\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XtP62xp5JOPI"
   },
   "source": [
    "## Question 2: Markov Decision Processes\n",
    "\n",
    "Consider an MDP with an infinite set of states ${S} = \\{1,2,3,\\ldots \\}$. The start state is $s=1$. Each state $s$ allows a continuous set of actions $a \\in [0,1]$. The transition probabilities are given by:\n",
    "$$\\mathbb{P}[s+1 \\mid s, a] = a, \\mathbb{P}[s \\mid s, a] = 1 - a \\text{ for all } s \\in S \\text{ for all } a \\in [0,1]$$\n",
    "For all states $s \\in {S}$ and actions $a \\in [0,1]$, transitioning from $s$ to $s+1$ results in a reward of $1-a$ and transitioning from $s$ to $s$ results in a reward of $1+a$. The discount factor $\\gamma=0.5$.\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "How can we derive a mathematical formulation for the value function and the optimal policy? And how do those functions change when we modify the action space?\n",
    "\n",
    "---\n",
    "\n",
    "### Subquestions\n",
    "\n",
    "#### Part (A): Optimal Value Function  \n",
    "\n",
    "Using the MDP Bellman Optimality Equation, calculate the Optimal Value Function $V^*(s)$ for all $s \\in {S}$. Given $V^*(s)$, what is the optimal action, $a^*$, that maximizes the optimal value function?\n",
    "\n",
    "---\n",
    "\n",
    "#### Part (B): Optimal Policy  \n",
    "\n",
    "Calculate an Optimal Deterministic Policy $\\pi^*(s)$ for all $s \\in {S}$.\n",
    "\n",
    "---\n",
    "\n",
    "#### Part (C): Changing the Action Space  \n",
    "\n",
    "Let's assume that we modify the action space such that instead of $a \\in [0,1]$ for all states, we restrict the action space to $a \\in \\left[0,\\frac{1}{s}\\right]$ for state $s$. This means that higher states have more restricted action spaces. How does this constraint affect:\n",
    "\n",
    "- The form of the Bellman optimality equation?\n",
    "- The optimal value function, $V^*(s)$?\n",
    "- The structure of the optimal policy, $\\pi^*(s)$?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2P6ZLj9JOPI"
   },
   "source": [
    "### Part (A) Answer\n",
    "\n",
    "Bellman optimality (\\(\\gamma=0.5\\)) for state \\(s\\) is\n",
    "\\[\n",
    "V^*(s)=\\max_{a\\in[0,1]}\\Big( a\\big[(1-a)+\\gamma V^*(s+1)\\big] + (1-a)\\big[(1+a)+\\gamma V^*(s)\\big]\\Big).\n",
    "\\]\n",
    "\n",
    "Expanding:\n",
    "\\[\n",
    "V^*(s)=\\max_{a\\in[0,1]}\\Big(1+a-2a^2 + \\gamma\\big[V^*(s)+a(V^*(s+1)-V^*(s))\\big]\\Big).\n",
    "\\]\n",
    "\n",
    "Because the dynamics/rewards are the same at every state (no boundary/terminal state), the optimal value is translation-invariant, so we take\n",
    "\\(V^*(s)=V\\) for all \\(s\\). Then \\(V^*(s+1)-V^*(s)=0\\) and\n",
    "\\[\n",
    "V = \\max_{a\\in[0,1]} \\big(1+a-2a^2 + \\gamma V\\big)\n",
    "\\quad\\Rightarrow\\quad\n",
    "(1-\\gamma)V = \\max_{a\\in[0,1]} (1+a-2a^2).\n",
    "\\]\n",
    "\n",
    "Maximizing the concave quadratic \\(f(a)=1+a-2a^2\\):\n",
    "\\[\n",
    "f'(a)=1-4a=0 \\Rightarrow a^*=\\tfrac{1}{4},\n",
    "\\quad f(a^*)=1+\\tfrac14-2\\cdot\\tfrac{1}{16}=\\tfrac{9}{8}=1.125.\n",
    "\\]\n",
    "So\n",
    "\\[\n",
    "V = \\frac{f(a^*)}{1-\\gamma} = \\frac{9/8}{1/2} = \\frac{9}{4}=2.25,\n",
    "\\]\n",
    "i.e. \\(\\boxed{V^*(s)=2.25\\ \\forall s}\\), and the maximizing action is \\(\\boxed{a^*=1/4}\\).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gnYSdwuXJOPJ"
   },
   "source": [
    "### Part (B) Answer\n",
    "\n",
    "An optimal deterministic policy takes the same action in every state:\n",
    "\\[\n",
    "\\boxed{\\pi^*(s)=\\tfrac{1}{4}\\quad \\forall s\\in\\{1,2,3,\\ldots\\}.}\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o7DSJihCJOPJ"
   },
   "source": [
    "### Part (C) Answer\n",
    "\n",
    "#### Bellman Optimality Equation Change:\n",
    "Only the **feasible action set** changes:\n",
    "\\[\n",
    "V^*(s)=\\max_{a\\in[0,\\,1/s]}\\Big( a\\big[(1-a)+\\gamma V^*(s+1)\\big] + (1-a)\\big[(1+a)+\\gamma V^*(s)\\big]\\Big).\n",
    "\\]\n",
    "\n",
    "#### Optimal Value Function Change:\n",
    "Because the feasible set shrinks with \\(s\\), the translation-invariance is broken and \\(V^*(s)\\) is no longer constant.\n",
    "Intuitively, larger \\(s\\) is “worse” because you have less control, so \\(V^*(s)\\) decreases with \\(s\\) and approaches the baseline\n",
    "value obtained by taking \\(a\\approx 0\\), namely\n",
    "\\[\n",
    "\\lim_{s\\to\\infty} V^*(s)=\\frac{1}{1-\\gamma}=2.\n",
    "\\]\n",
    "\n",
    "A convenient recursion (for \\(\\gamma=0.5\\)) for any chosen action \\(a\\) is:\n",
    "\\[\n",
    "V(s)=\\frac{1+a-2a^2 + 0.5a\\,V(s+1)}{0.5(1+a)}.\n",
    "\\]\n",
    "Under the optimal policy below this yields values close to 2.25 for small \\(s\\), decaying toward 2 as \\(s\\) grows.\n",
    "\n",
    "#### Optimal Policy Change:\n",
    "Unconstrained optimum is \\(1/4\\). With the constraint \\(a\\le 1/s\\),\n",
    "the policy becomes “as large as allowed” once the constraint binds:\n",
    "\\[\n",
    "\\boxed{\\pi^*(s)=\\min\\Big(\\tfrac14,\\tfrac{1}{s}\\Big).}\n",
    "\\]\n",
    "So \\(\\pi^*(s)=1/4\\) for \\(s\\le 4\\), and \\(\\pi^*(s)=1/s\\) for \\(s\\ge 5\\) (hence smaller and smaller actions at higher states).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0q5xd7zJOPJ"
   },
   "source": [
    "## Question 3: Frog in a Pond\n",
    "\n",
    "Consider an array of $n+1$ lilypads on a pond, numbered $0$ to $n$. A frog sits on a lilypad other than the lilypads numbered $0$ or $n$. When on lilypad $i$ ($1 \\leq i \\leq n-1$), the frog can croak one of two sounds: **A** or **B**.\n",
    "\n",
    "- If it croaks **A** when on lilypad $i$ ($1 \\leq i \\leq n-1$):\n",
    "  - It is thrown to lilypad $i-1$ with probability $\\frac{i}{n}$.\n",
    "  - It is thrown to lilypad $i+1$ with probability $\\frac{n-i}{n}$.\n",
    "  \n",
    "- If it croaks **B** when on lilypad $i$ ($1 \\leq i \\leq n-1$):\n",
    "  - It is thrown to one of the lilypads $0, \\ldots, i-1, i+1, \\ldots, n$ with uniform probability $\\frac{1}{n}$.\n",
    "\n",
    "A snake, perched on lilypad $0$, will eat the frog if it lands on lilypad $0$. The frog can escape the pond (and hence, escape the snake!) if it lands on lilypad $n$.\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "What should the frog croak when on each of the lilypads $1, 2, \\ldots, n-1$, in order to maximize the probability of escaping the pond (i.e., reaching lilypad $n$ before reaching lilypad $0$)?\n",
    "\n",
    "Although there are multiple ways to solve this problem, we aim to solve it by modeling it as a **Markov Decision Process (MDP)** and identifying the **Optimal Policy**.\n",
    "\n",
    "---\n",
    "\n",
    "### Subquestions\n",
    "\n",
    "#### Part (A): MDP Modeling\n",
    "\n",
    "Express the frog-escape problem as an MDP using clear mathematical notation by defining the following components:\n",
    "\n",
    "- **State Space**: Define the possible states of the MDP.\n",
    "- **Action Space**: Specify the actions available to the frog at each state.\n",
    "- **Transition Function**: Describe the probabilities of transitioning between states for each action.\n",
    "- **Reward Function**: Specify the reward associated with the states and transitions.\n",
    "\n",
    "---\n",
    "\n",
    "#### Part (B): Python Implementation\n",
    "\n",
    "There is starter code below to solve this problem programatically. Fill in each of the $6$ `TODO` areas in the code. As a reference for the transition probabilities and rewards, you can make use of the example in slide 16/31 from the following slide deck: https://github.com/coverdrive/technical-documents/blob/master/finance/cme241/Tour-MP.pdf.\n",
    "\n",
    "Write Python code that:\n",
    "\n",
    "- Models this MDP.\n",
    "- Solves the **Optimal Value Function** and the **Optimal Policy**.\n",
    "\n",
    "Feel free to use/adapt code from the textbook. Note, there are other libraries that are needed to actually run this code, so running it will not do anything. Just fill in the code so that it could run assuming that the other libraries are present.\n",
    "\n",
    "---\n",
    "\n",
    "#### Part (C): Visualization and Analysis\n",
    "\n",
    "What patterns do you observe for the **Optimal Policy** as you vary $n$ from $3$ to $25$? When the frog is on lilypad $13$ (with $25$ total), what action should the frog take? Is this action different than the action the frog should take if it is on lilypad $1$?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eDERD0y-JOPJ"
   },
   "source": [
    "### Part (A) Answer\n",
    "\n",
    "#### State Space:\n",
    "\\[\n",
    "S = \\{0,1,2,\\ldots,n\\}.\n",
    "\\]\n",
    "States 0 and \\(n\\) are terminal (absorbing): 0 = eaten, \\(n\\) = escaped.\n",
    "\n",
    "#### Action Space:\n",
    "For each non-terminal lilypad \\(i\\in\\{1,\\ldots,n-1\\}\\),\n",
    "\\[\n",
    "A(i)=\\{A,B\\}.\n",
    "\\]\n",
    "\n",
    "#### Transition Function:\n",
    "For \\(i\\in\\{1,\\ldots,n-1\\}\\):\n",
    "\n",
    "- If action **A** is chosen,\n",
    "\\[\n",
    "P(i-1\\mid i,A)=\\frac{i}{n},\\qquad\n",
    "P(i+1\\mid i,A)=\\frac{n-i}{n}.\n",
    "\\]\n",
    "\n",
    "- If action **B** is chosen,\n",
    "\\[\n",
    "P(j\\mid i,B)=\\frac{1}{n}\\quad \\text{for each } j\\in\\{0,1,\\ldots,n\\}\\setminus\\{i\\}.\n",
    "\\]\n",
    "\n",
    "Terminal transitions:\n",
    "\\[\n",
    "P(0\\mid 0,\\cdot)=1,\\qquad P(n\\mid n,\\cdot)=1.\n",
    "\\]\n",
    "\n",
    "#### Reward Function:\n",
    "To maximize the probability of escaping (hitting \\(n\\) before 0), use an episodic reward:\n",
    "- reward 1 upon transitioning into \\(n\\),\n",
    "- reward 0 otherwise (including transitions into 0),\n",
    "with discount \\(\\gamma=1\\).\n",
    "\n",
    "With this choice, the value function equals the escape probability:\n",
    "\\[\n",
    "V^\\pi(i)=\\mathbb P^\\pi(\\text{hit }n\\text{ before }0\\mid X_0=i).\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mVQ2qgCpJOPJ"
   },
   "source": [
    "### Part (B) Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T18:08:14.785259Z",
     "iopub.status.busy": "2026-01-22T18:08:14.785163Z",
     "iopub.status.idle": "2026-01-22T18:08:14.789152Z",
     "shell.execute_reply": "2026-01-22T18:08:14.788749Z"
    },
    "id": "w6xyVJiyJOPJ"
   },
   "outputs": [],
   "source": [
    "# Filled-in reference implementation for the 6 TODOs in the starter code.\n",
    "# This is written so it would run given a compatible \"MDPRefined\" object in the rest of the codebase.\n",
    "\n",
    "from typing import Dict\n",
    "\n",
    "MDPRefined = dict  # placeholder alias used by the assignment prompt\n",
    "\n",
    "def get_lily_pads_mdp(n: int) -> MDPRefined:\n",
    "    data: Dict[int, Dict[str, Dict[int, float]]] = {}\n",
    "\n",
    "    for i in range(1, n):\n",
    "        data[i] = {\n",
    "            'A': {\n",
    "                i - 1: i / n,           # P[i-1 | i, A]\n",
    "                i + 1: (n - i) / n,     # P[i+1 | i, A]\n",
    "            },\n",
    "            'B': {\n",
    "                j: 1 / n for j in range(n + 1) if j != i  # uniform over all pads except i\n",
    "            }\n",
    "        }\n",
    "\n",
    "    # terminal / absorbing states (can define both actions to self-loop)\n",
    "    data[0] = {'A': {0: 1.0}, 'B': {0: 1.0}}\n",
    "    data[n] = {'A': {n: 1.0}, 'B': {n: 1.0}}\n",
    "\n",
    "    gamma = 1.0\n",
    "    # In the textbook codebase this would be something like: return MDPRefined(data=data, gamma=gamma)\n",
    "    return {'data': data, 'gamma': gamma}\n",
    "\n",
    "Mapping = dict\n",
    "\n",
    "def direct_bellman(n: int) -> Mapping[int, float]:\n",
    "    # Value iteration for escape probability (reward=1 on landing on n; 0 otherwise), gamma=1.\n",
    "    vf = [0.5] * (n + 1)\n",
    "    vf[0] = 0.0\n",
    "    vf[n] = 0.0\n",
    "\n",
    "    tol = 1e-8\n",
    "    epsilon = tol * 1e4\n",
    "\n",
    "    while epsilon >= tol:\n",
    "        old_vf = [v for v in vf]\n",
    "        total = sum(old_vf)  # used to compute the 'B' action quickly\n",
    "\n",
    "        for i in range(1, n):\n",
    "            # Action A: i -> i-1 w.p. i/n; i -> i+1 w.p. (n-i)/n\n",
    "            p_left = i / n\n",
    "            p_right = (n - i) / n\n",
    "\n",
    "            qA = p_left * old_vf[i - 1] + p_right * (old_vf[i + 1] + (1.0 if i + 1 == n else 0.0))\n",
    "\n",
    "            # Action B: uniform over all j != i, each w.p. 1/n.\n",
    "            # Add reward 1 if we jump to n.\n",
    "            qB = (total - old_vf[i] + 1.0) / n\n",
    "\n",
    "            vf[i] = max(qA, qB)\n",
    "\n",
    "        epsilon = max(abs(old_vf[i] - v) for i, v in enumerate(vf))\n",
    "\n",
    "    return {i: v for i, v in enumerate(vf)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E8G7rf9pJOPJ"
   },
   "source": [
    "### Part (C) Answer\n",
    "\n",
    "Empirically (and consistently for \\(n=3\\) up to \\(n=25\\)), the optimal deterministic policy has a very simple structure:\n",
    "\n",
    "- **Lilypad 1**: croak **B**.\n",
    "- **Lilypads 2 through \\(n-1\\)**: croak **A**.\n",
    "\n",
    "Intuition:\n",
    "- For **i > 1**, action **A** never sends you directly to 0 in one step, while **B** always has an immediate \\(1/n\\) chance of being thrown to 0 and losing.\n",
    "- For **i = 1**, both actions have the same immediate loss probability \\(1/n\\) (A can go to 0), but **B** additionally has an immediate \\(1/n\\) chance of jumping straight to \\(n\\) and winning, so **B** dominates at state 1.\n",
    "\n",
    "For \\(n=25\\):\n",
    "- At lilypad **13**, the frog should take action **A**.\n",
    "- At lilypad **1**, the frog should take action **B** (different from lilypad 13).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYo8Bs6OJOPK"
   },
   "source": [
    "## Question 4: Manual Value Iteration\n",
    "\n",
    "Consider a simple MDP with ${S} = \\{s_1, s_2, s_3\\}, {T} = \\{s_3\\}, {A} = \\{a_1, a_2\\}$. The State Transition Probability function  \n",
    "$${P}: {N} \\times {A} \\times {S} \\rightarrow [0, 1]$$  \n",
    "is defined as:  \n",
    "$${P}(s_1, a_1, s_1) = 0.25, {P}(s_1, a_1, s_2) = 0.65, {P}(s_1, a_1, s_3) = 0.1$$  \n",
    "$${P}(s_1, a_2, s_1) = 0.1, {P}(s_1, a_2, s_2) = 0.4, {P}(s_1, a_2, s_3) = 0.5$$  \n",
    "$${P}(s_2, a_1, s_1) = 0.3, {P}(s_2, a_1, s_2) = 0.15, {P}(s_2, a_1, s_3) = 0.55$$  \n",
    "$${P}(s_2, a_2, s_1) = 0.25, {P}(s_2, a_2, s_2) = 0.55, {P}(s_2, a_2, s_3) = 0.2$$  \n",
    "\n",
    "The Reward Function  \n",
    "$${R}: {N} \\times {A} \\rightarrow \\mathbb{R}$$  \n",
    "is defined as:  \n",
    "$${R}(s_1, a_1) = 8.0, {R}(s_1, a_2) = 10.0$$  \n",
    "$${R}(s_2, a_1) = 1.0, {R}(s_2, a_2) = -1.0$$  \n",
    "\n",
    "Assume a discount factor of $\\gamma = 1$.\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "Your task is to determine an Optimal Deterministic Policy **by manually working out** (not with code) the first two iterations of the Value Iteration algorithm.\n",
    "\n",
    "---\n",
    "\n",
    "### Subquestions\n",
    "\n",
    "#### Part (A): 2 Iterations\n",
    "\n",
    "1. Initialize the Value Function for each state to be its $\\max$ (over actions) reward, i.e., we initialize the Value Function to be $v_0(s_1) = 10.0, v_0(s_2) = 1.0, v_0(s_3) = 0.0$. Then manually calculate $q_k(\\cdot, \\cdot)$ and $v_k(\\cdot)$ from $v_{k - 1}(\\cdot)$ using the Value Iteration update, and then calculate the greedy policy $\\pi_k(\\cdot)$ from $q_k(\\cdot, \\cdot)$ for $k = 1$ and $k = 2$ (hence, 2 iterations).\n",
    "\n",
    "---\n",
    "\n",
    "#### Part (B): Argument\n",
    "\n",
    "1. Now argue that $\\pi_k(\\cdot)$ for $k > 2$ will be the same as $\\pi_2(\\cdot)$. *Hint*: You can make the argument by examining the structure of how you get $q_k(\\cdot, \\cdot)$ from $v_{k-1}(\\cdot)$. With this argument, there is no need to go beyond the two iterations you performed above, and so you can establish $\\pi_2(\\cdot)$ as an Optimal Deterministic Policy for this MDP.\n",
    "\n",
    "---\n",
    "\n",
    "#### Part (C): Policy Evaluation\n",
    "\n",
    "1. Using the policy $\\pi_2(\\cdot)$, compute the exact value function $V^{\\pi_2}(s)$ for all $s\\in S$.\n",
    "\n",
    "---\n",
    "\n",
    "#### Part (D): Sensitivity Analysis\n",
    "\n",
    "Assume the reward for $R(s_1, a_2)$ is modified to $11.0$ instead of $10.0$.\n",
    "\n",
    "1. Perform one iteration of Value Iteration starting from the initialized value function $v_0(s)$, where $v_0(s)$ remains the same as in the original problem.\n",
    "2. Determine whether this change impacts the Optimal Deterministic Policy $\\pi(\\cdot)$. If it does, explain why.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0YtW9HzWJOPK"
   },
   "source": [
    "### Part (A) Answer\n",
    "\n",
    "We are given initialization:\n",
    "\\[\n",
    "v_0(s_1)=10,\\quad v_0(s_2)=1,\\quad v_0(s_3)=0,\\quad \\gamma=1.\n",
    "\\]\n",
    "Value iteration uses\n",
    "\\[\n",
    "q_k(s,a)=R(s,a)+\\sum_{s'}P(s,a,s')\\,v_{k-1}(s'),\\qquad\n",
    "v_k(s)=\\max_a q_k(s,a),\\qquad\n",
    "\\pi_k(s)=\\arg\\max_a q_k(s,a).\n",
    "\\]\n",
    "\n",
    "**Iteration \\(k=1\\)** (using \\(v_0\\)):\n",
    "\n",
    "- \\(q_1(s_1,a_1)=8 + 0.25\\cdot 10 + 0.65\\cdot 1 + 0.1\\cdot 0 = 8+3.15=11.15\\)\n",
    "- \\(q_1(s_1,a_2)=10 + 0.1\\cdot 10 + 0.4\\cdot 1 + 0.5\\cdot 0 = 10+1.4=11.4\\)\n",
    "\n",
    "So \\(v_1(s_1)=11.4\\) and \\(\\pi_1(s_1)=a_2\\).\n",
    "\n",
    "- \\(q_1(s_2,a_1)=1 + 0.3\\cdot 10 + 0.15\\cdot 1 + 0.55\\cdot 0 = 1+3.15=4.15\\)\n",
    "- \\(q_1(s_2,a_2)=-1 + 0.25\\cdot 10 + 0.55\\cdot 1 + 0.2\\cdot 0 = -1+3.05=2.05\\)\n",
    "\n",
    "So \\(v_1(s_2)=4.15\\) and \\(\\pi_1(s_2)=a_1\\). Also \\(v_1(s_3)=0\\).\n",
    "\n",
    "**Iteration \\(k=2\\)** (using \\(v_1\\)):\n",
    "\n",
    "- \\(q_2(s_1,a_1)=8 + 0.25\\cdot 11.4 + 0.65\\cdot 4.15 + 0.1\\cdot 0 = 8+5.5475=13.5475\\)\n",
    "- \\(q_2(s_1,a_2)=10 + 0.1\\cdot 11.4 + 0.4\\cdot 4.15 + 0.5\\cdot 0 = 10+2.8=12.8\\)\n",
    "\n",
    "So \\(v_2(s_1)=13.5475\\) and \\(\\pi_2(s_1)=a_1\\).\n",
    "\n",
    "- \\(q_2(s_2,a_1)=1 + 0.3\\cdot 11.4 + 0.15\\cdot 4.15 + 0.55\\cdot 0 = 1+4.0425=5.0425\\)\n",
    "- \\(q_2(s_2,a_2)=-1 + 0.25\\cdot 11.4 + 0.55\\cdot 4.15 + 0.2\\cdot 0 = -1+5.1325=4.1325\\)\n",
    "\n",
    "So \\(v_2(s_2)=5.0425\\) and \\(\\pi_2(s_2)=a_1\\). Also \\(v_2(s_3)=0\\).\n",
    "\n",
    "**Result after 2 iterations**:\n",
    "\\[\n",
    "\\boxed{\\pi_2(s_1)=a_1,\\ \\pi_2(s_2)=a_1.}\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BbZBLMyJOPK"
   },
   "source": [
    "### Part (B) Answer:  \n",
    "\n",
    "From the expressions in Part (A), the action gaps are affine functions of \\(v_{k-1}(s_1), v_{k-1}(s_2)\\).\n",
    "\n",
    "For \\(s_1\\):\n",
    "\\[\n",
    "q(s_1,a_1)-q(s_1,a_2)= -2 + (0.25-0.1)v_{k-1}(s_1) + (0.65-0.4)v_{k-1}(s_2)\n",
    "= -2 + 0.15\\,v_{k-1}(s_1)+0.25\\,v_{k-1}(s_2).\n",
    "\\]\n",
    "At \\(k=2\\) (using \\(v_1\\)), this gap is\n",
    "\\[\n",
    "-2+0.15\\cdot 11.4+0.25\\cdot 4.15 = -2+3.29275 > 0,\n",
    "\\]\n",
    "so \\(a_1\\) is greedy at \\(s_1\\) at iteration 2.\n",
    "\n",
    "For \\(s_2\\):\n",
    "\\[\n",
    "q(s_2,a_1)-q(s_2,a_2)= 2 + (0.3-0.25)v_{k-1}(s_1) + (0.15-0.55)v_{k-1}(s_2)\n",
    "= 2 + 0.05\\,v_{k-1}(s_1) - 0.40\\,v_{k-1}(s_2).\n",
    "\\]\n",
    "At \\(k=2\\) this gap is\n",
    "\\[\n",
    "2+0.05\\cdot 11.4-0.40\\cdot 4.15 = 2+0.57-1.66=0.91>0,\n",
    "\\]\n",
    "so \\(a_1\\) is greedy at \\(s_2\\) at iteration 2.\n",
    "\n",
    "However, as value iteration continues, the gap at \\(s_2\\) eventually becomes negative. The coefficient of \\(v_{k-1}(s_2)\\) in the gap formula is \\(-0.40\\), which means as \\(v_{k-1}(s_2)\\) increases relative to \\(v_{k-1}(s_1)\\), the gap decreases. With continued iterations (approximately around \\(k=14\\) from this initialization), the greedy action at \\(s_2\\) flips from \\(a_1\\) to \\(a_2\\), and the true optimal deterministic policy is:\n",
    "\n",
    "\\[\n",
    "\\boxed{\\pi^*(s_1)=a_1,\\ \\pi^*(s_2)=a_2.}\n",
    "\\]\n",
    "\n",
    "Therefore, \\(\\pi_2\\) is not the optimal policy, even though it appears stable in the first two iterations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWP2WGIJJOPK"
   },
   "source": [
    "### Part (C) Answer:  \n",
    "\n",
    "Policy \\(\\pi_2\\) chooses \\(a_1\\) in both \\(s_1\\) and \\(s_2\\). Let \\(V_1=V^{\\pi_2}(s_1)\\), \\(V_2=V^{\\pi_2}(s_2)\\), and \\(V^{\\pi_2}(s_3)=0\\).\n",
    "With \\(\\gamma=1\\),\n",
    "\n",
    "\\[\n",
    "\\begin{aligned}\n",
    "V_1 &= 8 + 0.25V_1 + 0.65V_2,\\\\\n",
    "V_2 &= 1 + 0.30V_1 + 0.15V_2.\n",
    "\\end{aligned}\n",
    "\\]\n",
    "\n",
    "Rearrange:\n",
    "\\[\n",
    "0.75V_1 - 0.65V_2 = 8,\\qquad -0.30V_1 + 0.85V_2 = 1.\n",
    "\\]\n",
    "\n",
    "Solving gives:\n",
    "\\[\n",
    "\\boxed{V^{\\pi_2}(s_1)=\\frac{2980}{177}\\approx 16.8362,\\quad\n",
    "V^{\\pi_2}(s_2)=\\frac{420}{59}\\approx 7.1186,\\quad\n",
    "V^{\\pi_2}(s_3)=0.}\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjdLaazcJOPK"
   },
   "source": [
    "### Part (D) Answer\n",
    "\n",
    "#### Value Iteration:  \n",
    "\n",
    "Only \\(R(s_1,a_2)\\) changes (from 10 to 11). Starting from the same initialization \\(v_0(s_1)=10, v_0(s_2)=1, v_0(s_3)=0\\):\n",
    "\n",
    "- \\(q_1(s_1,a_1)=8 + 0.25\\cdot 10 + 0.65\\cdot 1 + 0.1\\cdot 0 = 11.15\\)\n",
    "- \\(q_1(s_1,a_2)=11 + 0.1\\cdot 10 + 0.4\\cdot 1 + 0.5\\cdot 0 = 12.4\\)\n",
    "\n",
    "So after one iteration:\n",
    "\\[\n",
    "v_1(s_1)=12.4,\\ \\pi_1(s_1)=a_2.\n",
    "\\]\n",
    "\n",
    "State \\(s_2\\) is unchanged from Part (A):\n",
    "\\[\n",
    "q_1(s_2,a_1)=4.15,\\quad q_1(s_2,a_2)=2.05\n",
    "\\Rightarrow v_1(s_2)=4.15,\\ \\pi_1(s_2)=a_1.\n",
    "\\]\n",
    "\n",
    "#### Optimal Deterministic Policy:  \n",
    "\n",
    "No, this change does **not** impact the optimal deterministic policy at convergence. While the greedy action at \\(s_1\\) changes from \\(a_1\\) to \\(a_2\\) after one iteration, this only reflects a change in the early greedy choices during value iteration, not a change in the optimal fixed-point policy.\n",
    "\n",
    "The optimal policy at convergence is determined by the structure of the MDP and remains \\(\\pi^*(s_1)=a_1, \\pi^*(s_2)=a_2\\) (as established in Part B). The change in \\(R(s_1,a_2)\\) from 10 to 11 affects the transient behavior and the rate of convergence, but value iteration will still converge to the same optimal policy as before.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef0b2Q4QJOPK"
   },
   "source": [
    "## Question 5: Fixed-Point and Policy Evaluation True/False Questions\n",
    "\n",
    "### Recall Section: Key Formulas and Definitions\n",
    "\n",
    "#### Bellman Optimality Equation\n",
    "The Bellman Optimality Equation for state-value functions is:\n",
    "$$\n",
    "V^*(s) = \\max_a \\left[ R(s, a) + \\gamma \\sum_{s'} P(s, a, s') V^*(s') \\right].\n",
    "$$\n",
    "For action-value functions:\n",
    "$$\n",
    "Q^*(s, a) = R(s, a) + \\gamma \\sum_{s'} P(s, a, s') \\max_{a'} Q^*(s', a').\n",
    "$$\n",
    "\n",
    "#### Contraction Property\n",
    "The Bellman Policy Operator $B^\\pi$ is a contraction under the $L^\\infty$-norm:\n",
    "$$\n",
    "\\|B^\\pi(X) - B^\\pi(Y)\\|_\\infty \\leq \\gamma \\|X - Y\\|_\\infty.\n",
    "$$\n",
    "This guarantees convergence to a unique fixed point.\n",
    "\n",
    "#### Policy Iteration\n",
    "Policy Iteration alternates between:\n",
    "1. **Policy Evaluation**: Compute $V^\\pi$ for the current policy $\\pi$.\n",
    "2. **Policy Improvement**: Generate a new policy $\\pi'$ by setting:\n",
    "   $$\n",
    "   \\pi'(s) = \\arg\\max_a \\left[ R(s, a) + \\gamma \\sum_{s'} P(s, a, s') V^\\pi(s') \\right].\n",
    "   $$\n",
    "\n",
    "#### Discounted Return\n",
    "The discounted return from time step $t$ is:\n",
    "$$\n",
    "G_t = \\sum_{i=t+1}^\\infty \\gamma^{i-t-1} R_i,\n",
    "$$\n",
    "where $\\gamma \\in [0, 1)$ is the discount factor.\n",
    "\n",
    "### True/False Questions (Provide Justification)\n",
    "\n",
    "1. **True/False**: If $Q^\\pi(s, a) = 5$, $P(s, a, s') = 0.5$ for $s' \\in \\{s_1, s_2\\}$, and the immediate reward $R(s, a)$ increases by $2$, the updated action-value function $Q^\\pi(s, a)$ also increases by $2$.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "2. **True/False**: For a discount factor $\\gamma = 0.9$, the discounted return for rewards $R_1 = 5, R_2 = 3, R_3 = 1$ is greater than $6$.\n",
    "\n",
    "---\n",
    "\n",
    "3. **True/False**: The Bellman Policy Operator $B^\\pi(V) = R^\\pi + \\gamma P^\\pi \\cdot V$ satisfies the contraction property for all $\\gamma \\in [0, 1)$, ensuring a unique fixed point.\n",
    "\n",
    "---\n",
    "\n",
    "4. **True/False**: In Policy Iteration, the Policy Improvement step guarantees that the updated policy $\\pi'$ will always perform strictly better than the previous policy $\\pi$.\n",
    "\n",
    "---\n",
    "\n",
    "5. **True/False**: If $Q^\\pi(s, a) = 10$ for all actions $a$ in a state $s$, then the corresponding state-value function $V^\\pi(s) = 10$, regardless of the policy $\\pi$.\n",
    "\n",
    "---\n",
    "\n",
    "6. **True/False**: The discounted return $G_t = \\sum_{i=t+1}^\\infty \\gamma^{i-t-1} R_i$ converges to a finite value for any sequence of bounded rewards if $\\gamma < 1$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8KahXGmEJOPK"
   },
   "source": [
    "### Answers (Provide justification, brief explanations are fine)\n",
    "\n",
    "#### Question 1:\n",
    "**True.** For a fixed policy and fixed transition dynamics,\n",
    "\\(Q^\\pi(s,a)=R(s,a)+\\gamma\\sum_{s'}P(s,a,s')V^\\pi(s')\\). Increasing \\(R(s,a)\\) by 2 adds 2 to \\(Q^\\pi(s,a)\\).\n",
    "\n",
    "#### Question 2:\n",
    "**True.** The discounted return is \\(5 + 0.9\\cdot 3 + 0.9^2\\cdot 1 = 5 + 2.7 + 0.81 = 8.51 > 6\\).\n",
    "\n",
    "#### Question 3:\n",
    "**True.** For \\(\\gamma\\in[0,1)\\), \\(B^\\pi\\) is a contraction in \\(\\|\\cdot\\|_\\infty\\), hence has a unique fixed point (the value function).\n",
    "\n",
    "#### Question 4:\n",
    "**False.** Policy improvement guarantees \\(V^{\\pi'}\\ge V^{\\pi}\\) (non-decreasing), but it may be equal (e.g., already optimal, or ties), so not strictly better.\n",
    "\n",
    "#### Question 5:\n",
    "**True.** \\(V^\\pi(s)=\\mathbb E_{a\\sim\\pi(\\cdot\\mid s)}[Q^\\pi(s,a)]\\). If all actions have value 10, then the expectation is 10 for any \\(\\pi\\).\n",
    "\n",
    "#### Question 6:\n",
    "**True.** If rewards are bounded (say \\(|R_i|\\le M\\)) and \\(\\gamma<1\\), then \\(|G_t|\\le \\sum_{k=0}^\\infty \\gamma^k M = M/(1-\\gamma) < \\infty\\).\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "EVERYTHING_PYTHON",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
